{
    "lstm": {
        "seq_len": 16,
        "n_features": 16,
        "hidden_size": 64,
        "latent_size": 32,
        "num_layers": 1,
        "dropout": 0.0
    },
    "gru": {
        "input_dim": 16,
        "hidden_dim": 64,
        "latent_dim": 32,
        "num_layers": 1,
        "dropout": 0.0,
        "bidirectional": false
    },
    "cnn": {
        "seq_len": 16,
        "in_channels": 16,
        "channels": [
            32,
            64,
            128
        ],
        "kernel_sizes": [
            3,
            3,
            3
        ],
        "dilations": [
            1,
            2,
            4
        ],
        "use_residual": true,
        "norm": "batch",
        "act": "relu",
        "dropout": 0.1,
        "up_mode": "transpose",
        "latent_dim": 64
    }
}